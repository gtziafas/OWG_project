<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Towards Open-World Grasping with Large Vision-Language Models">
  <meta property="og:title" content="Towards Open-World Grasping with Large Vision-Language Models"/>
  <meta property="og:description" content="'Towards Open-World Grasping with Large Vision-Language Models' project website, CoRL 2024."/>
  <meta property="og:url" content="https://gtziafas.github.io/OWG_project/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Towards Open-World Grasping with Large Vision-Language Models">
  <meta name="twitter:description" content="'Towards Open-World Grasping with Large Vision-Language Models' project website, CoRL 2024">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Towards Open-World Grasping with Large Vision-Language Models</h1>    
	     <!-- Paper authors -->
             <div class="is-size-4 publication-authors">
    		    <span class="author-block" style="color: darkred;">Conference on Robot Learning 2024</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
              </div>	  
  
	      <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://github.com/gtziafas" target="_blank">Georgios Tziafas </a>,</span>
                <span class="author-block">
                  <a href="https://hkasaei.github.io/" target="_blank">Hamidreza Kasaei</a>,</span>
                  </div>

                  <div class="is-size-8 publication-authors">
                    <span class="author-block">
                      <a href="https://www.ai.rug.nl/irl-lab/" target="_blank">Interactive Robot Learning Lab</a><br>Department of Artificial Intelligence, University of Groningen</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Paper link -->
                    <span class="link-block">
                      <a href="static/pdfs/OWG_CORL24_paper.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/OWG_CORL24_appendix.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/gtziafas/OWG" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- YouTube video Link -->
               <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=<YOUTUBE_VIDEO_ID>" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
               	  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
	       </span>


                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.18722v2" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
	
<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/owg_teaser.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           The ability to grasp objects in-the-wild from open-ended language instructions constitutes a fundamental challenge in robotics. An open-world grasping system should be able to combine high-level contextual with low-level physical-geometric reasoning in order to be applicable in arbitrary scenarios. Recent works exploit the web-scale knowledge inherent in large language models (LLMs) to plan and reason in robotic context, but rely on external vision and
action models to ground such knowledge into the environment and parameterize actuation. This setup suffers from two major bottlenecks: a) the LLM’s reasoning capacity is constrained by the quality of visual grounding, and b) LLMs do not contain low-level spatial understanding of the world, which is essential for grasping in contact-rich scenarios. In this work we demonstrate that modern vision-language models (VLMs) are capable of tackling such limitations, as they are implicitly grounded and can jointly reason about semantics and geometry. We propose OWG, an open-world grasping pipeline that combines VLMs with segmentation and grasp synthesis models to unlock grounded world understanding in three stages: open-ended referring segmentation, grounded grasp planning and grasp ranking via contact reasoning, all of which can be applied zero-shot via suitable visual prompting mechanisms. We conduct extensive evaluation in cluttered indoor scene datasets to showcase OWG’s robustness in grounding from open-ended language, as well as open-world robotic grasping experiments in both simulation and hardware that demonstrate superior performance compared to previous supervised and zero-shot LLM-based methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Method image -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Open-World Grasper (OWG)</h2>
      <div class="columns is-centered has-text-centered">
       <div class="item">
        <!-- Your image here -->
          <img src="static/images/fdsfad.drawio.png" alt="alt text" style="margin-top: 20px;" />
          <h2 class="subtitle has-text-justified" style="font-size: 1rem; margin-bottom: 25px;">
         Given a user instruction and an observation, OWG first invokes a segmentation model to recover pixel-level masks, and overlays them with numeric IDs as visual markers in a new image. Then the VLM subsequently activates three stages: (i) grounding the target object from the language expression in the marked image, (ii) planning on whether it should grasp the target or remove a surrounding object, and (iii) invoking a grasp synthesis model to generate grasps and ranking them according to the object's shape and neighbouring information. The best grasp pose (highlighted here in pink - not part of the prompt) is executed and the observation is updated for a new run, until the target object is grasped.human user provides an instruction and a language parser synthesizes an executable program (bottom left), built out of a primitives library (bottom middle). A program executor utilises a set of concept grounding modules to ground words to different objects (center) and executes the predicted program step-by-step (top right), in order to identify the queried object and instruct the robot to grasp it (bottom right).
        </h2>
      </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End method image -->


<!-- Components response image -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Zero-Shot Mark-based Visual Prompting</h2>
      <div class="columns is-centered has-text-centered">
       <div class="item">
        <!-- Your image here -->
          <img src="static/images/ShowAll.drawio.png" alt="alt text" style="margin-top: 20px; width: 90%; height: auto;" />
          <h2 class="subtitle has-text-justified" style="font-size: 1rem; margin-bottom: 25px;">
        Example GPT-4v responses (from left to right): a) Open-ended referring segmentation, i.e., grounding, b) Grounded grasp planning, and c) Grasp ranking via contact reasoning. We omit parts of the prompt and response for brievity. We draw visual markers such as instance segmentations and grasp proposals, along with unique label IDs, which enables GPT-4v to reason about the objects in natural language by referring to their IDs.
        </h2>
      </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End components response image --

<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Robot Experiments</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/OWGwebsiteRobotExp1.mp4"
            type="video/mp4">
          </video>
	  <h2 class="subtitle has-text-centered" style="margin-top: 10px;">
            Grasping in isolated scenarios.
          </h2>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/OWGwebsiteRobotExp2.mp4"
            type="video/mp4">
          </video>
	  <h2 class="subtitle has-text-centered" style="margin-top: 10px;">
            Grasping in cluttered scenarios. OWG first removes blocking objects to make the target more graspable.
          </h2>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/OWGwebsiteRobotExp3.mp4"
            type="video/mp4">
         </video>
	   <h2 class="subtitle has-text-centered" style="margin-top: 10px;">
            Recovery from failures / external disturbances.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->


<!-- Grounding exp table -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Robustness to Open-Ended Language</h2>
      <div class="columns is-centered has-text-centered">
       <div class="item">
        <!-- Your image here -->
          <img src="static/images/tab1.png" alt="alt text" style="margin-top: 20px; width: 75%; height: auto;" />
          <h2 class="subtitle has-text-justified" style="font-size: 0.85rem; margin-top: 15px; margin-bottom: 15px;">
In order to evaluate the open-ended potential of OWG for grounding, we curate the OCID dataset for referring expressions of different types: a) <strong>Name</strong>, which contains an open-vocabulary description of an object (e.g. <i>Choco Krispies</i>), b) <strong>Attribute</strong>, which described a specific object attribute such as color, texture, shape, material or state (e.g. <i>opened cereal box</i>), c) <strong>spatial / visual / semantic relations</strong>, which contain multi-object references that describe relations between objects (e.g. <i>cereal behind bowl</i>),  d) <strong>affordances</strong>, which contain a verb indicating the user's intent, requiring contextual understanding to resolve (i.e. <i>I want to wipe my hands</i>), and e) <strong>multi-hop</strong>, which contain multiple rounds of reasoning step to reach a final answer (e.g. <i>second largest cereal box from the left of the blue bowl</i>).
We find that GPT-4v, when prompted with explicit marker-based visual prompts can outperform previous CLIP-based methods, as well as other visually-grounded open-source VLMs.         
	  </h2>
      </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End grounding exp table -->


<!-- Grounding image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
          <img src="static/images/RSSExampleResponses_1.png" alt="MY ALT TEXT" style="width: 75%; display: block; margin: 20px auto;" />
        <h2 class="subtitle has-text-centered">
            Grounding open-vocabulary object names and attributes.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
          <img src="static/images/RSSExampleResponses_2.png" alt="MY ALT TEXT" style="width: 75%; display: block; margin: 20px auto;" />
        <h2 class="subtitle has-text-centered">
          Grounding complex relations between objects.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
          <img src="static/images/RSSExampleResponses_3.png" alt="MY ALT TEXT" style="width: 75%; display: block; margin: 20px auto;" />
        <h2 class="subtitle has-text-centered">
          Grounding user affordances with contextual semantics.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End grounding image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{tziafas2024openworldgraspinglargevisionlanguage,
      title={Towards Open-World Grasping with Large Vision-Language Models}, 
      author={Georgios Tziafas and Hamidreza Kasaei},
      year={2024},
      journal={8th Conference on Robot Learning (CoRL 2024)},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
